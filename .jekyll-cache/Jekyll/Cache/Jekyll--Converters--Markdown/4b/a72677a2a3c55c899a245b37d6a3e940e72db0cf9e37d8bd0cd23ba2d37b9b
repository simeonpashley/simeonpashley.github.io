I"O-<p>Some notes from another round of performance optimisation of our server stack hosting our online platform that dramatically improved customer experience too.</p>

<!-- more -->

<p>The environment is a set of dedicated front end webhosts running Nginx &amp; PHP-FPM all of which share a common disk volume for the web application itself. Services including MySQL, Memcached and Elastic Search run on seperate VMs to ease the load. Memcached exists in 2 forms:</p>

<p>1 - a dedicated <strong>session</strong> cache.</p>

<p>2 - a dedicated <strong>content</strong> cache.</p>

<p>There are also <a href="https://www.ukfast.co.uk/web-acceleration.html">bespoke Varnish caches</a> sat infront of the servers to reduce load as much as possible.</p>

<p>Each web VM is running 1 instant of NGinx forwarding to a local PHP-FPM farm of 2 local ports to improve core use and reduce latency. APC is running locally to improve PHP raw performance.</p>

<p>In this configuration disk writes are <strong>very</strong> expensive as they lock the shared volume and force a sync across all reading devices.</p>

<h3 id="the-scenario">The scenario</h3>

<p>An extreme load event was expected when the business appeared was to appear on National TV. The load was expected to be 10x-20x the highest number of concurrent users we’d seen.</p>

<p>Here are a few strategies taken along the way.</p>

<h3 id="losing-session-data">Losing Session Data</h3>

<p>The first issue encountered when load testing was that session data was getting lost, where users were experiencing random
The issue was that when PHP-FPM was left to it’s own session management it appeared to lazily write out the session data at some point after the PHP page work closed. This causes issues in a multi-threaded layout.</p>

<p>Lets imagine 2 servers, A &amp; B, now lets imagine each one runs 2 PHP-FPM works, 1 &amp; 2. As a user retrieves content from the servers they could retrieve it from anywhere at random A1, A2, B1 &amp; B2. Now lets imagine that content from A1 updates session data but B2 starts reading before A1 has had the chance to write it’s data out. B2 doesn’t see the update and may write it’s own verson of session data.</p>

<p><strong>The answer is simple:</strong> Explcitly add <code class="language-plaintext highlighter-rouge">session_write_close();</code> into your PHP code before <code class="language-plaintext highlighter-rouge">exit;</code> to write the session data out and immediately close the PHP worker.</p>

<h3 id="exclusive-cache-content-creation">Exclusive cache content creation</h3>

<p>When a cached component expires it must be rebuilt, then re-cached. If multiple requests are made simultaneously this means that each requests can attempt to rebuild the data &amp; re-cache the data itself.</p>

<p>We use two strategies to eliminate this issue:</p>

<ul>
  <li>
    <p>Use server <strong>cron jobs</strong> to pro-actively &amp; forceably refresh the content before the cache expires. E.g., the homepage has a natural 5 min expiry on it but the cron job refreshes this data every 1 min so the clients never rebuild the content.</p>
  </li>
  <li>
    <p>When a rebuild occurs due to cache expiry there is a <strong>Mutex</strong> lock placed around the build code.</p>
  </li>
</ul>

<p>– Option 1: If the build is locked then that build is passed over, typically returning no content. Typically a content build is microseconds so there could be a handful of users that get blank content.</p>

<p>– Option 2: If the build is locked then wait to acquire the mutex and rebuild the content. This approach always returns content but causes long waits &amp; multiple rebuilds for all requests.</p>

<h3 id="is-the-cache-working">Is the cache working?</h3>

<p>Our stack uses 2 layers of caching: 1) Varnish as the front layer, 2) Memcached for fragments.</p>

<p>When the site is under load it’s difficult to tell just what isn’t being cached and is being accessed.</p>

<p>The following command shows which of the files are being read at the moment of issuing the command,</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> --><td class="rouge-code"><pre><span class="nv">$ </span>lsof | <span class="nb">grep</span> <span class="nt">-e</span> <span class="s2">"[[:digit:]]</span><span class="se">\+</span><span class="s2">r"</span> | <span class="nb">grep</span> /var/www/vhost
</pre></td></tr></tbody></table></code></pre></div></div>

<p>As well as the expected access of PHP, this exposed some image file accessing that should have been cached by Varnish and never actually reached the backend server.</p>

<h3 id="gfs2-contention-issues">GFS2 contention issues</h3>

<p>In the environment used, there is a shared disk volume using GFS2 that houses the content for the websites. We already know that accessing this shared disk is expensive and writing to it is very very expensive.</p>

<p>A common occurence is maxed out CPU usage on the web servers that typically comes down to <code class="language-plaintext highlighter-rouge">glock_workqueue</code> holding 100% CPU while it waits for disk access.</p>

<p>This redhat article also describes the issue we had pretty much exactly: https://access.redhat.com/solutions/69822</p>

<blockquote>
  <p>When a large number of cached glocks are built up for GFS2, and memory pressure causes a flush of cache, the CPU utilization of <code class="language-plaintext highlighter-rouge">glock_workqueue</code> becomes very high, possibly causing the system to become unresponsive.</p>

  <p>When page cached is flushed out, the CPU utilization of <code class="language-plaintext highlighter-rouge">glock_workqueue</code> spikes.</p>

  <p>We are seeing high load on our clusters. This high load is due to CPU utilization. There is an associated large drop in page cache for the GFS2 filesystem during the high load. When the large pagecache drop completes, the load goes back down to normal. The <code class="language-plaintext highlighter-rouge">glock_workqueue</code> processes are increased during this high load time period. Very little I/O occurs during the high load event.</p>

  <p>A hung process was halted, and during that time the system stopped functioning for all existing users. A glock service spiked to 100% CPU, you were not able to start any new ssh sessions and it kicked existing ssh users off. The symptom went away by the time you we were able to gain access… We saw a huge spike in load and CPU usage, but I/O was at normal levels.</p>

  <p>Multiple instances of glock_workqueue using 100% cpu.</p>
</blockquote>

<p>While trying to find this issue I found that <code class="language-plaintext highlighter-rouge">iotop</code> was far too generic and didn’t give the detail I really neded to find this issue.</p>

<p>In our scenario the shared volume was mounted under <code class="language-plaintext highlighter-rouge">/var/www/vhosts</code> so to discover which files weere open for writing I used the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> --><td class="rouge-code"><pre>lsof | <span class="nb">grep</span> <span class="nt">-e</span> <span class="s2">"[[:digit:]]</span><span class="se">\+</span><span class="s2">w"</span> | <span class="nb">grep</span> /var/www/vhost
</pre></td></tr></tbody></table></code></pre></div></div>

<p>What came to light was that the <code class="language-plaintext highlighter-rouge">nginx</code> process was locking every file for write access! Wow!!</p>

<h4 id="remove-access-time-updates">Remove access time updates</h4>

<p>Whilst investigating glock contention further I found this article that may be useful with reference to reducing the file writes:
https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Global_File_System_2/s1-ov-lockbounce.html</p>

<blockquote>
  <p>If you do not set the “noatime mount” parameter, then reads will also result in writes to update the file timestamps. We recommend that all GFS2 users should mount with noatime unless they have a specific requirement for
atime.”</p>
</blockquote>

<p>Our engineers took this a step further and implemented <code class="language-plaintext highlighter-rouge">noatime nodiratime</code> on the shared volume mount, which instantly removed the disk write access and <code class="language-plaintext highlighter-rouge">glock_workqueue</code> disappeared from iotop.</p>

<h3 id="reducing-general-disk-access">Reducing general disk access</h3>

<p>In an effort to further reduce disk access system wide there were a few small changes made to various services.</p>

<h4 id="apc">APC</h4>

<p><a href="http://php.net/manual/en/book.apc.php">APC cache</a> will access the disk for 2 things, writing temporary files and <code class="language-plaintext highlighter-rouge">stat</code>ing the files. <code class="language-plaintext highlighter-rouge">stat</code> means to check the file timestamp to see if it has been modified.</p>

<p>Use a ram-disk for storing temporary files, in our stack this is <code class="language-plaintext highlighter-rouge">/dev/shm</code></p>

<p><strong>apc.ini</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> --><td class="rouge-code"><pre>apc.mmap_file_mask=/dev/shm/apc.XXXXXX
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Disable <code class="language-plaintext highlighter-rouge">stat</code>ing your code to see if it’s changed. <strong>BEWARE</strong> that APC will now need telling to flush the cache when you update/deploy your code or you won’t see the changes. I achieve this via a small php script that is invoked on each server when I deploy new code by using <code class="language-plaintext highlighter-rouge">wget</code> in my deploy scripts.</p>

<p><strong>apc.ini</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> --><td class="rouge-code"><pre>apc.stat=0
</pre></td></tr></tbody></table></code></pre></div></div>

<p><strong>apc-clear.php</strong></p>

<div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><!-- <td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td> --><td class="rouge-code"><pre><span class="nb">apc_clear_cache</span><span class="p">();</span>
<span class="nb">apc_clear_cache</span><span class="p">(</span><span class="s1">'opcode'</span><span class="p">);</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h4 id="new-relic">New Relic</h4>

<p>While <a href="http://newrelic.com">New Relic is a peerless awesome server analyis tool</a>, it does write a lot of log files out. In short, find &amp; disable all of the log files you can. The New Relic eco-system is so variable and changes so rapidly that it’s pointless me describing the specifics here but here’s some I had to change.</p>

<ul>
  <li>nr-sysmon-d</li>
  <li>nr-nginx</li>
  <li>nr-plugin (php-fpm)</li>
  <li>nr-mysql</li>
</ul>
:ET